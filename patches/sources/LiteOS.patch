diff --git a/arch/arm/common/cmsis/cmsis_gcc.h b/arch/arm/common/cmsis/cmsis_gcc.h
index 2d9db15a..9d8e671d 100644
--- a/arch/arm/common/cmsis/cmsis_gcc.h
+++ b/arch/arm/common/cmsis/cmsis_gcc.h
@@ -128,7 +128,7 @@
  */
 __STATIC_FORCEINLINE void __enable_irq(void)
 {
-  __ASM volatile ("cpsie i" : : : "memory");
+// __ASM volatile ("cpsie i" : : : "memory");
 }
 
 
@@ -139,7 +139,7 @@ __STATIC_FORCEINLINE void __enable_irq(void)
  */
 __STATIC_FORCEINLINE void __disable_irq(void)
 {
-  __ASM volatile ("cpsid i" : : : "memory");
+// __ASM volatile ("cpsid i" : : : "memory");
 }
 
 
@@ -152,7 +152,7 @@ __STATIC_FORCEINLINE uint32_t __get_CONTROL(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, control" : "=r" (result) );
+// __ASM volatile ("MRS %0, control" : "=r" (result) );
   return(result);
 }
 
@@ -167,7 +167,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_CONTROL_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, control_ns" : "=r" (result) );
+// __ASM volatile ("MRS %0, control_ns" : "=r" (result) );
   return(result);
 }
 #endif
@@ -180,7 +180,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_CONTROL_NS(void)
  */
 __STATIC_FORCEINLINE void __set_CONTROL(uint32_t control)
 {
-  __ASM volatile ("MSR control, %0" : : "r" (control) : "memory");
+// __ASM volatile ("MSR control, %0" : : "r" (control) : "memory");
 }
 
 
@@ -192,7 +192,7 @@ __STATIC_FORCEINLINE void __set_CONTROL(uint32_t control)
  */
 __STATIC_FORCEINLINE void __TZ_set_CONTROL_NS(uint32_t control)
 {
-  __ASM volatile ("MSR control_ns, %0" : : "r" (control) : "memory");
+// __ASM volatile ("MSR control_ns, %0" : : "r" (control) : "memory");
 }
 #endif
 
@@ -206,7 +206,7 @@ __STATIC_FORCEINLINE uint32_t __get_IPSR(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, ipsr" : "=r" (result) );
+// __ASM volatile ("MRS %0, ipsr" : "=r" (result) );
   return(result);
 }
 
@@ -220,7 +220,7 @@ __STATIC_FORCEINLINE uint32_t __get_APSR(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, apsr" : "=r" (result) );
+// __ASM volatile ("MRS %0, apsr" : "=r" (result) );
   return(result);
 }
 
@@ -234,7 +234,7 @@ __STATIC_FORCEINLINE uint32_t __get_xPSR(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, xpsr" : "=r" (result) );
+// __ASM volatile ("MRS %0, xpsr" : "=r" (result) );
   return(result);
 }
 
@@ -248,7 +248,7 @@ __STATIC_FORCEINLINE uint32_t __get_PSP(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, psp"  : "=r" (result) );
+// __ASM volatile ("MRS %0, psp"  : "=r" (result) );
   return(result);
 }
 
@@ -263,7 +263,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_PSP_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, psp_ns"  : "=r" (result) );
+// __ASM volatile ("MRS %0, psp_ns"  : "=r" (result) );
   return(result);
 }
 #endif
@@ -276,7 +276,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_PSP_NS(void)
  */
 __STATIC_FORCEINLINE void __set_PSP(uint32_t topOfProcStack)
 {
-  __ASM volatile ("MSR psp, %0" : : "r" (topOfProcStack) : );
+// __ASM volatile ("MSR psp, %0" : : "r" (topOfProcStack) : );
 }
 
 
@@ -288,7 +288,7 @@ __STATIC_FORCEINLINE void __set_PSP(uint32_t topOfProcStack)
  */
 __STATIC_FORCEINLINE void __TZ_set_PSP_NS(uint32_t topOfProcStack)
 {
-  __ASM volatile ("MSR psp_ns, %0" : : "r" (topOfProcStack) : );
+// __ASM volatile ("MSR psp_ns, %0" : : "r" (topOfProcStack) : );
 }
 #endif
 
@@ -302,7 +302,7 @@ __STATIC_FORCEINLINE uint32_t __get_MSP(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, msp" : "=r" (result) );
+// __ASM volatile ("MRS %0, msp" : "=r" (result) );
   return(result);
 }
 
@@ -317,7 +317,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_MSP_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, msp_ns" : "=r" (result) );
+// __ASM volatile ("MRS %0, msp_ns" : "=r" (result) );
   return(result);
 }
 #endif
@@ -330,7 +330,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_MSP_NS(void)
  */
 __STATIC_FORCEINLINE void __set_MSP(uint32_t topOfMainStack)
 {
-  __ASM volatile ("MSR msp, %0" : : "r" (topOfMainStack) : );
+ // __ASM volatile ("MSR msp, %0" : : "r" (topOfMainStack) : );
 }
 
 
@@ -342,7 +342,7 @@ __STATIC_FORCEINLINE void __set_MSP(uint32_t topOfMainStack)
  */
 __STATIC_FORCEINLINE void __TZ_set_MSP_NS(uint32_t topOfMainStack)
 {
-  __ASM volatile ("MSR msp_ns, %0" : : "r" (topOfMainStack) : );
+// __ASM volatile ("MSR msp_ns, %0" : : "r" (topOfMainStack) : );
 }
 #endif
 
@@ -357,7 +357,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_SP_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, sp_ns" : "=r" (result) );
+// __ASM volatile ("MRS %0, sp_ns" : "=r" (result) );
   return(result);
 }
 
@@ -369,7 +369,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_SP_NS(void)
  */
 __STATIC_FORCEINLINE void __TZ_set_SP_NS(uint32_t topOfStack)
 {
-  __ASM volatile ("MSR sp_ns, %0" : : "r" (topOfStack) : );
+// __ASM volatile ("MSR sp_ns, %0" : : "r" (topOfStack) : );
 }
 #endif
 
@@ -383,7 +383,7 @@ __STATIC_FORCEINLINE uint32_t __get_PRIMASK(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, primask" : "=r" (result) :: "memory");
+// __ASM volatile ("MRS %0, primask" : "=r" (result) :: "memory");
   return(result);
 }
 
@@ -398,7 +398,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_PRIMASK_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, primask_ns" : "=r" (result) :: "memory");
+// __ASM volatile ("MRS %0, primask_ns" : "=r" (result) :: "memory");
   return(result);
 }
 #endif
@@ -411,7 +411,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_PRIMASK_NS(void)
  */
 __STATIC_FORCEINLINE void __set_PRIMASK(uint32_t priMask)
 {
-  __ASM volatile ("MSR primask, %0" : : "r" (priMask) : "memory");
+// __ASM volatile ("MSR primask, %0" : : "r" (priMask) : "memory");
 }
 
 
@@ -423,7 +423,7 @@ __STATIC_FORCEINLINE void __set_PRIMASK(uint32_t priMask)
  */
 __STATIC_FORCEINLINE void __TZ_set_PRIMASK_NS(uint32_t priMask)
 {
-  __ASM volatile ("MSR primask_ns, %0" : : "r" (priMask) : "memory");
+// __ASM volatile ("MSR primask_ns, %0" : : "r" (priMask) : "memory");
 }
 #endif
 
@@ -438,7 +438,7 @@ __STATIC_FORCEINLINE void __TZ_set_PRIMASK_NS(uint32_t priMask)
  */
 __STATIC_FORCEINLINE void __enable_fault_irq(void)
 {
-  __ASM volatile ("cpsie f" : : : "memory");
+// __ASM volatile ("cpsie f" : : : "memory");
 }
 
 
@@ -449,7 +449,7 @@ __STATIC_FORCEINLINE void __enable_fault_irq(void)
  */
 __STATIC_FORCEINLINE void __disable_fault_irq(void)
 {
-  __ASM volatile ("cpsid f" : : : "memory");
+// __ASM volatile ("cpsid f" : : : "memory");
 }
 
 
@@ -462,7 +462,7 @@ __STATIC_FORCEINLINE uint32_t __get_BASEPRI(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, basepri" : "=r" (result) );
+// __ASM volatile ("MRS %0, basepri" : "=r" (result) );
   return(result);
 }
 
@@ -477,7 +477,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_BASEPRI_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, basepri_ns" : "=r" (result) );
+// __ASM volatile ("MRS %0, basepri_ns" : "=r" (result) );
   return(result);
 }
 #endif
@@ -490,7 +490,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_BASEPRI_NS(void)
  */
 __STATIC_FORCEINLINE void __set_BASEPRI(uint32_t basePri)
 {
-  __ASM volatile ("MSR basepri, %0" : : "r" (basePri) : "memory");
+// __ASM volatile ("MSR basepri, %0" : : "r" (basePri) : "memory");
 }
 
 
@@ -502,7 +502,7 @@ __STATIC_FORCEINLINE void __set_BASEPRI(uint32_t basePri)
  */
 __STATIC_FORCEINLINE void __TZ_set_BASEPRI_NS(uint32_t basePri)
 {
-  __ASM volatile ("MSR basepri_ns, %0" : : "r" (basePri) : "memory");
+// __ASM volatile ("MSR basepri_ns, %0" : : "r" (basePri) : "memory");
 }
 #endif
 
@@ -515,7 +515,7 @@ __STATIC_FORCEINLINE void __TZ_set_BASEPRI_NS(uint32_t basePri)
  */
 __STATIC_FORCEINLINE void __set_BASEPRI_MAX(uint32_t basePri)
 {
-  __ASM volatile ("MSR basepri_max, %0" : : "r" (basePri) : "memory");
+// __ASM volatile ("MSR basepri_max, %0" : : "r" (basePri) : "memory");
 }
 
 
@@ -528,7 +528,7 @@ __STATIC_FORCEINLINE uint32_t __get_FAULTMASK(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, faultmask" : "=r" (result) );
+// __ASM volatile ("MRS %0, faultmask" : "=r" (result) );
   return(result);
 }
 
@@ -543,7 +543,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_FAULTMASK_NS(void)
 {
   uint32_t result;
 
-  __ASM volatile ("MRS %0, faultmask_ns" : "=r" (result) );
+// __ASM volatile ("MRS %0, faultmask_ns" : "=r" (result) );
   return(result);
 }
 #endif
@@ -556,7 +556,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_FAULTMASK_NS(void)
  */
 __STATIC_FORCEINLINE void __set_FAULTMASK(uint32_t faultMask)
 {
-  __ASM volatile ("MSR faultmask, %0" : : "r" (faultMask) : "memory");
+// __ASM volatile ("MSR faultmask, %0" : : "r" (faultMask) : "memory");
 }
 
 
@@ -568,7 +568,7 @@ __STATIC_FORCEINLINE void __set_FAULTMASK(uint32_t faultMask)
  */
 __STATIC_FORCEINLINE void __TZ_set_FAULTMASK_NS(uint32_t faultMask)
 {
-  __ASM volatile ("MSR faultmask_ns, %0" : : "r" (faultMask) : "memory");
+// __ASM volatile ("MSR faultmask_ns, %0" : : "r" (faultMask) : "memory");
 }
 #endif
 
@@ -597,7 +597,7 @@ __STATIC_FORCEINLINE uint32_t __get_PSPLIM(void)
   return 0U;
 #else
   uint32_t result;
-  __ASM volatile ("MRS %0, psplim"  : "=r" (result) );
+// __ASM volatile ("MRS %0, psplim"  : "=r" (result) );
   return result;
 #endif
 }
@@ -618,7 +618,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_PSPLIM_NS(void)
   return 0U;
 #else
   uint32_t result;
-  __ASM volatile ("MRS %0, psplim_ns"  : "=r" (result) );
+// __ASM volatile ("MRS %0, psplim_ns"  : "=r" (result) );
   return result;
 #endif
 }
@@ -641,7 +641,7 @@ __STATIC_FORCEINLINE void __set_PSPLIM(uint32_t ProcStackPtrLimit)
   // without main extensions, the non-secure PSPLIM is RAZ/WI
   (void)ProcStackPtrLimit;
 #else
-  __ASM volatile ("MSR psplim, %0" : : "r" (ProcStackPtrLimit));
+// __ASM volatile ("MSR psplim, %0" : : "r" (ProcStackPtrLimit));
 #endif
 }
 
@@ -661,7 +661,7 @@ __STATIC_FORCEINLINE void __TZ_set_PSPLIM_NS(uint32_t ProcStackPtrLimit)
   // without main extensions, the non-secure PSPLIM is RAZ/WI
   (void)ProcStackPtrLimit;
 #else
-  __ASM volatile ("MSR psplim_ns, %0\n" : : "r" (ProcStackPtrLimit));
+// __ASM volatile ("MSR psplim_ns, %0\n" : : "r" (ProcStackPtrLimit));
 #endif
 }
 #endif
@@ -684,7 +684,7 @@ __STATIC_FORCEINLINE uint32_t __get_MSPLIM(void)
   return 0U;
 #else
   uint32_t result;
-  __ASM volatile ("MRS %0, msplim" : "=r" (result) );
+// __ASM volatile ("MRS %0, msplim" : "=r" (result) );
   return result;
 #endif
 }
@@ -706,7 +706,7 @@ __STATIC_FORCEINLINE uint32_t __TZ_get_MSPLIM_NS(void)
   return 0U;
 #else
   uint32_t result;
-  __ASM volatile ("MRS %0, msplim_ns" : "=r" (result) );
+// __ASM volatile ("MRS %0, msplim_ns" : "=r" (result) );
   return result;
 #endif
 }
@@ -729,7 +729,7 @@ __STATIC_FORCEINLINE void __set_MSPLIM(uint32_t MainStackPtrLimit)
   // without main extensions, the non-secure MSPLIM is RAZ/WI
   (void)MainStackPtrLimit;
 #else
-  __ASM volatile ("MSR msplim, %0" : : "r" (MainStackPtrLimit));
+// __ASM volatile ("MSR msplim, %0" : : "r" (MainStackPtrLimit));
 #endif
 }
 
@@ -749,7 +749,7 @@ __STATIC_FORCEINLINE void __TZ_set_MSPLIM_NS(uint32_t MainStackPtrLimit)
   // without main extensions, the non-secure MSPLIM is RAZ/WI
   (void)MainStackPtrLimit;
 #else
-  __ASM volatile ("MSR msplim_ns, %0" : : "r" (MainStackPtrLimit));
+// __ASM volatile ("MSR msplim_ns, %0" : : "r" (MainStackPtrLimit));
 #endif
 }
 #endif
@@ -775,7 +775,7 @@ __STATIC_FORCEINLINE uint32_t __get_FPSCR(void)
 #else
   uint32_t result;
 
-  __ASM volatile ("VMRS %0, fpscr" : "=r" (result) );
+// __ASM volatile ("VMRS %0, fpscr" : "=r" (result) );
   return(result);
 #endif
 #else
@@ -799,7 +799,7 @@ __STATIC_FORCEINLINE void __set_FPSCR(uint32_t fpscr)
   /* see https://gcc.gnu.org/ml/gcc-patches/2017-04/msg00443.html */
   __builtin_arm_set_fpscr(fpscr);
 #else
-  __ASM volatile ("VMSR fpscr, %0" : : "r" (fpscr) : "vfpcc", "memory");
+// __ASM volatile ("VMSR fpscr, %0" : : "r" (fpscr) : "vfpcc", "memory");
 #endif
 #else
   (void)fpscr;
@@ -833,13 +833,13 @@ __STATIC_FORCEINLINE void __set_FPSCR(uint32_t fpscr)
   \brief   No Operation
   \details No Operation does nothing. This instruction can be used for code alignment purposes.
  */
-#define __NOP()                             __ASM volatile ("nop")
+#define __NOP()                           // __ASM volatile ("nop")
 
 /**
   \brief   Wait For Interrupt
   \details Wait For Interrupt is a hint instruction that suspends execution until one of a number of events occurs.
  */
-#define __WFI()                             __ASM volatile ("wfi")
+#define __WFI()                           // __ASM volatile ("wfi")
 
 
 /**
@@ -847,14 +847,14 @@ __STATIC_FORCEINLINE void __set_FPSCR(uint32_t fpscr)
   \details Wait For Event is a hint instruction that permits the processor to enter
            a low-power state until one of a number of events occurs.
  */
-#define __WFE()                             __ASM volatile ("wfe")
+#define __WFE()                           // __ASM volatile ("wfe")
 
 
 /**
   \brief   Send Event
   \details Send Event is a hint instruction. It causes an event to be signaled to the CPU.
  */
-#define __SEV()                             __ASM volatile ("sev")
+#define __SEV()                           // __ASM volatile ("sev")
 
 
 /**
@@ -865,7 +865,7 @@ __STATIC_FORCEINLINE void __set_FPSCR(uint32_t fpscr)
  */
 __STATIC_FORCEINLINE void __ISB(void)
 {
-  __ASM volatile ("isb 0xF":::"memory");
+// __ASM volatile ("isb 0xF":::"memory");
 }
 
 
@@ -876,7 +876,7 @@ __STATIC_FORCEINLINE void __ISB(void)
  */
 __STATIC_FORCEINLINE void __DSB(void)
 {
-  __ASM volatile ("dsb 0xF":::"memory");
+// __ASM volatile ("dsb 0xF":::"memory");
 }
 
 
@@ -887,7 +887,7 @@ __STATIC_FORCEINLINE void __DSB(void)
  */
 __STATIC_FORCEINLINE void __DMB(void)
 {
-  __ASM volatile ("dmb 0xF":::"memory");
+// __ASM volatile ("dmb 0xF":::"memory");
 }
 
 
@@ -904,7 +904,7 @@ __STATIC_FORCEINLINE uint32_t __REV(uint32_t value)
 #else
   uint32_t result;
 
-  __ASM volatile ("rev %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
+// __ASM volatile ("rev %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
   return result;
 #endif
 }
@@ -920,7 +920,7 @@ __STATIC_FORCEINLINE uint32_t __REV16(uint32_t value)
 {
   uint32_t result;
 
-  __ASM volatile ("rev16 %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
+// __ASM volatile ("rev16 %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
   return result;
 }
 
@@ -938,7 +938,7 @@ __STATIC_FORCEINLINE int16_t __REVSH(int16_t value)
 #else
   int16_t result;
 
-  __ASM volatile ("revsh %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
+// __ASM volatile ("revsh %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
   return result;
 #endif
 }
@@ -969,7 +969,7 @@ __STATIC_FORCEINLINE uint32_t __ROR(uint32_t op1, uint32_t op2)
   \param [in]    value  is ignored by the processor.
                  If required, a debugger can use it to store additional information about the breakpoint.
  */
-#define __BKPT(value)                       __ASM volatile ("bkpt "#value)
+#define __BKPT(value)                     // __ASM volatile ("bkpt "#value)
 
 
 /**
@@ -985,7 +985,7 @@ __STATIC_FORCEINLINE uint32_t __RBIT(uint32_t value)
 #if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \
      (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \
      (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1))    )
-   __ASM volatile ("rbit %0, %1" : "=r" (result) : "r" (value) );
+ // __ASM volatile ("rbit %0, %1" : "=r" (result) : "r" (value) );
 #else
   uint32_t s = (4U /*sizeof(v)*/ * 8U) - 1U; /* extra shift needed at end */
 
@@ -1026,12 +1026,12 @@ __STATIC_FORCEINLINE uint8_t __LDREXB(volatile uint8_t *addr)
     uint32_t result;
 
 #if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)
-   __ASM volatile ("ldrexb %0, %1" : "=r" (result) : "Q" (*addr) );
+ // __ASM volatile ("ldrexb %0, %1" : "=r" (result) : "Q" (*addr) );
 #else
     /* Prior to GCC 4.8, "Q" will be expanded to [rx, #0] which is not
        accepted by assembler. So has to use following less efficient pattern.
     */
-   __ASM volatile ("ldrexb %0, [%1]" : "=r" (result) : "r" (addr) : "memory" );
+ // __ASM volatile ("ldrexb %0, [%1]" : "=r" (result) : "r" (addr) : "memory" );
 #endif
    return ((uint8_t) result);    /* Add explicit type cast here */
 }
@@ -1048,12 +1048,12 @@ __STATIC_FORCEINLINE uint16_t __LDREXH(volatile uint16_t *addr)
     uint32_t result;
 
 #if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)
-   __ASM volatile ("ldrexh %0, %1" : "=r" (result) : "Q" (*addr) );
+ // __ASM volatile ("ldrexh %0, %1" : "=r" (result) : "Q" (*addr) );
 #else
     /* Prior to GCC 4.8, "Q" will be expanded to [rx, #0] which is not
        accepted by assembler. So has to use following less efficient pattern.
     */
-   __ASM volatile ("ldrexh %0, [%1]" : "=r" (result) : "r" (addr) : "memory" );
+ // __ASM volatile ("ldrexh %0, [%1]" : "=r" (result) : "r" (addr) : "memory" );
 #endif
    return ((uint16_t) result);    /* Add explicit type cast here */
 }
@@ -1069,7 +1069,7 @@ __STATIC_FORCEINLINE uint32_t __LDREXW(volatile uint32_t *addr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldrex %0, %1" : "=r" (result) : "Q" (*addr) );
+ // __ASM volatile ("ldrex %0, %1" : "=r" (result) : "Q" (*addr) );
    return(result);
 }
 
@@ -1086,7 +1086,7 @@ __STATIC_FORCEINLINE uint32_t __STREXB(uint8_t value, volatile uint8_t *addr)
 {
    uint32_t result;
 
-   __ASM volatile ("strexb %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("strexb %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" ((uint32_t)value) );
    return(result);
 }
 
@@ -1103,7 +1103,7 @@ __STATIC_FORCEINLINE uint32_t __STREXH(uint16_t value, volatile uint16_t *addr)
 {
    uint32_t result;
 
-   __ASM volatile ("strexh %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("strexh %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" ((uint32_t)value) );
    return(result);
 }
 
@@ -1120,7 +1120,7 @@ __STATIC_FORCEINLINE uint32_t __STREXW(uint32_t value, volatile uint32_t *addr)
 {
    uint32_t result;
 
-   __ASM volatile ("strex %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" (value) );
+ // __ASM volatile ("strex %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" (value) );
    return(result);
 }
 
@@ -1131,7 +1131,7 @@ __STATIC_FORCEINLINE uint32_t __STREXW(uint32_t value, volatile uint32_t *addr)
  */
 __STATIC_FORCEINLINE void __CLREX(void)
 {
-  __ASM volatile ("clrex" ::: "memory");
+// __ASM volatile ("clrex" ::: "memory");
 }
 
 #endif /* ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \
@@ -1154,7 +1154,7 @@ __STATIC_FORCEINLINE void __CLREX(void)
 __extension__ \
 ({                          \
   int32_t __RES, __ARG1 = (ARG1); \
-  __ASM ("ssat %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
+// __ASM ("ssat %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
   __RES; \
  })
 
@@ -1170,7 +1170,7 @@ __extension__ \
  __extension__ \
 ({                          \
   uint32_t __RES, __ARG1 = (ARG1); \
-  __ASM ("usat %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
+// __ASM ("usat %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
   __RES; \
  })
 
@@ -1186,7 +1186,7 @@ __STATIC_FORCEINLINE uint32_t __RRX(uint32_t value)
 {
   uint32_t result;
 
-  __ASM volatile ("rrx %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
+// __ASM volatile ("rrx %0, %1" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );
   return(result);
 }
 
@@ -1202,12 +1202,12 @@ __STATIC_FORCEINLINE uint8_t __LDRBT(volatile uint8_t *ptr)
     uint32_t result;
 
 #if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)
-   __ASM volatile ("ldrbt %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldrbt %0, %1" : "=r" (result) : "Q" (*ptr) );
 #else
     /* Prior to GCC 4.8, "Q" will be expanded to [rx, #0] which is not
        accepted by assembler. So has to use following less efficient pattern.
     */
-   __ASM volatile ("ldrbt %0, [%1]" : "=r" (result) : "r" (ptr) : "memory" );
+ // __ASM volatile ("ldrbt %0, [%1]" : "=r" (result) : "r" (ptr) : "memory" );
 #endif
    return ((uint8_t) result);    /* Add explicit type cast here */
 }
@@ -1224,12 +1224,12 @@ __STATIC_FORCEINLINE uint16_t __LDRHT(volatile uint16_t *ptr)
     uint32_t result;
 
 #if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)
-   __ASM volatile ("ldrht %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldrht %0, %1" : "=r" (result) : "Q" (*ptr) );
 #else
     /* Prior to GCC 4.8, "Q" will be expanded to [rx, #0] which is not
        accepted by assembler. So has to use following less efficient pattern.
     */
-   __ASM volatile ("ldrht %0, [%1]" : "=r" (result) : "r" (ptr) : "memory" );
+ // __ASM volatile ("ldrht %0, [%1]" : "=r" (result) : "r" (ptr) : "memory" );
 #endif
    return ((uint16_t) result);    /* Add explicit type cast here */
 }
@@ -1245,7 +1245,7 @@ __STATIC_FORCEINLINE uint32_t __LDRT(volatile uint32_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldrt %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldrt %0, %1" : "=r" (result) : "Q" (*ptr) );
    return(result);
 }
 
@@ -1258,7 +1258,7 @@ __STATIC_FORCEINLINE uint32_t __LDRT(volatile uint32_t *ptr)
  */
 __STATIC_FORCEINLINE void __STRBT(uint8_t value, volatile uint8_t *ptr)
 {
-   __ASM volatile ("strbt %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("strbt %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
 }
 
 
@@ -1270,7 +1270,7 @@ __STATIC_FORCEINLINE void __STRBT(uint8_t value, volatile uint8_t *ptr)
  */
 __STATIC_FORCEINLINE void __STRHT(uint16_t value, volatile uint16_t *ptr)
 {
-   __ASM volatile ("strht %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("strht %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
 }
 
 
@@ -1282,7 +1282,7 @@ __STATIC_FORCEINLINE void __STRHT(uint16_t value, volatile uint16_t *ptr)
  */
 __STATIC_FORCEINLINE void __STRT(uint32_t value, volatile uint32_t *ptr)
 {
-   __ASM volatile ("strt %1, %0" : "=Q" (*ptr) : "r" (value) );
+ // __ASM volatile ("strt %1, %0" : "=Q" (*ptr) : "r" (value) );
 }
 
 #else  /* ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \
@@ -1355,7 +1355,7 @@ __STATIC_FORCEINLINE uint8_t __LDAB(volatile uint8_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldab %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldab %0, %1" : "=r" (result) : "Q" (*ptr) );
    return ((uint8_t) result);
 }
 
@@ -1370,7 +1370,7 @@ __STATIC_FORCEINLINE uint16_t __LDAH(volatile uint16_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldah %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldah %0, %1" : "=r" (result) : "Q" (*ptr) );
    return ((uint16_t) result);
 }
 
@@ -1385,7 +1385,7 @@ __STATIC_FORCEINLINE uint32_t __LDA(volatile uint32_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("lda %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("lda %0, %1" : "=r" (result) : "Q" (*ptr) );
    return(result);
 }
 
@@ -1398,7 +1398,7 @@ __STATIC_FORCEINLINE uint32_t __LDA(volatile uint32_t *ptr)
  */
 __STATIC_FORCEINLINE void __STLB(uint8_t value, volatile uint8_t *ptr)
 {
-   __ASM volatile ("stlb %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("stlb %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
 }
 
 
@@ -1410,7 +1410,7 @@ __STATIC_FORCEINLINE void __STLB(uint8_t value, volatile uint8_t *ptr)
  */
 __STATIC_FORCEINLINE void __STLH(uint16_t value, volatile uint16_t *ptr)
 {
-   __ASM volatile ("stlh %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("stlh %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
 }
 
 
@@ -1422,7 +1422,7 @@ __STATIC_FORCEINLINE void __STLH(uint16_t value, volatile uint16_t *ptr)
  */
 __STATIC_FORCEINLINE void __STL(uint32_t value, volatile uint32_t *ptr)
 {
-   __ASM volatile ("stl %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("stl %1, %0" : "=Q" (*ptr) : "r" ((uint32_t)value) );
 }
 
 
@@ -1436,7 +1436,7 @@ __STATIC_FORCEINLINE uint8_t __LDAEXB(volatile uint8_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldaexb %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldaexb %0, %1" : "=r" (result) : "Q" (*ptr) );
    return ((uint8_t) result);
 }
 
@@ -1451,7 +1451,7 @@ __STATIC_FORCEINLINE uint16_t __LDAEXH(volatile uint16_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldaexh %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldaexh %0, %1" : "=r" (result) : "Q" (*ptr) );
    return ((uint16_t) result);
 }
 
@@ -1466,7 +1466,7 @@ __STATIC_FORCEINLINE uint32_t __LDAEX(volatile uint32_t *ptr)
 {
     uint32_t result;
 
-   __ASM volatile ("ldaex %0, %1" : "=r" (result) : "Q" (*ptr) );
+ // __ASM volatile ("ldaex %0, %1" : "=r" (result) : "Q" (*ptr) );
    return(result);
 }
 
@@ -1483,7 +1483,7 @@ __STATIC_FORCEINLINE uint32_t __STLEXB(uint8_t value, volatile uint8_t *ptr)
 {
    uint32_t result;
 
-   __ASM volatile ("stlexb %0, %2, %1" : "=&r" (result), "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("stlexb %0, %2, %1" : "=&r" (result), "=Q" (*ptr) : "r" ((uint32_t)value) );
    return(result);
 }
 
@@ -1500,7 +1500,7 @@ __STATIC_FORCEINLINE uint32_t __STLEXH(uint16_t value, volatile uint16_t *ptr)
 {
    uint32_t result;
 
-   __ASM volatile ("stlexh %0, %2, %1" : "=&r" (result), "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("stlexh %0, %2, %1" : "=&r" (result), "=Q" (*ptr) : "r" ((uint32_t)value) );
    return(result);
 }
 
@@ -1517,7 +1517,7 @@ __STATIC_FORCEINLINE uint32_t __STLEX(uint32_t value, volatile uint32_t *ptr)
 {
    uint32_t result;
 
-   __ASM volatile ("stlex %0, %2, %1" : "=&r" (result), "=Q" (*ptr) : "r" ((uint32_t)value) );
+ // __ASM volatile ("stlex %0, %2, %1" : "=&r" (result), "=Q" (*ptr) : "r" ((uint32_t)value) );
    return(result);
 }
 
@@ -1539,7 +1539,7 @@ __STATIC_FORCEINLINE uint32_t __SADD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("sadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("sadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1547,7 +1547,7 @@ __STATIC_FORCEINLINE uint32_t __QADD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("qadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1555,7 +1555,7 @@ __STATIC_FORCEINLINE uint32_t __SHADD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("shadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("shadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1563,7 +1563,7 @@ __STATIC_FORCEINLINE uint32_t __UADD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1571,7 +1571,7 @@ __STATIC_FORCEINLINE uint32_t __UQADD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uqadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uqadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1579,7 +1579,7 @@ __STATIC_FORCEINLINE uint32_t __UHADD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uhadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uhadd8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1588,7 +1588,7 @@ __STATIC_FORCEINLINE uint32_t __SSUB8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("ssub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("ssub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1596,7 +1596,7 @@ __STATIC_FORCEINLINE uint32_t __QSUB8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("qsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1604,7 +1604,7 @@ __STATIC_FORCEINLINE uint32_t __SHSUB8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("shsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("shsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1612,7 +1612,7 @@ __STATIC_FORCEINLINE uint32_t __USUB8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("usub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("usub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1620,7 +1620,7 @@ __STATIC_FORCEINLINE uint32_t __UQSUB8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uqsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uqsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1628,7 +1628,7 @@ __STATIC_FORCEINLINE uint32_t __UHSUB8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uhsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uhsub8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1637,7 +1637,7 @@ __STATIC_FORCEINLINE uint32_t __SADD16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("sadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("sadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1645,7 +1645,7 @@ __STATIC_FORCEINLINE uint32_t __QADD16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("qadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1653,7 +1653,7 @@ __STATIC_FORCEINLINE uint32_t __SHADD16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("shadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("shadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1661,7 +1661,7 @@ __STATIC_FORCEINLINE uint32_t __UADD16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1669,7 +1669,7 @@ __STATIC_FORCEINLINE uint32_t __UQADD16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uqadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uqadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1677,7 +1677,7 @@ __STATIC_FORCEINLINE uint32_t __UHADD16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uhadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uhadd16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1685,7 +1685,7 @@ __STATIC_FORCEINLINE uint32_t __SSUB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("ssub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("ssub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1693,7 +1693,7 @@ __STATIC_FORCEINLINE uint32_t __QSUB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("qsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1701,7 +1701,7 @@ __STATIC_FORCEINLINE uint32_t __SHSUB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("shsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("shsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1709,7 +1709,7 @@ __STATIC_FORCEINLINE uint32_t __USUB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("usub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("usub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1717,7 +1717,7 @@ __STATIC_FORCEINLINE uint32_t __UQSUB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uqsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uqsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1725,7 +1725,7 @@ __STATIC_FORCEINLINE uint32_t __UHSUB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uhsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uhsub16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1733,7 +1733,7 @@ __STATIC_FORCEINLINE uint32_t __SASX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("sasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("sasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1741,7 +1741,7 @@ __STATIC_FORCEINLINE uint32_t __QASX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("qasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1749,7 +1749,7 @@ __STATIC_FORCEINLINE uint32_t __SHASX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("shasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("shasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1757,7 +1757,7 @@ __STATIC_FORCEINLINE uint32_t __UASX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1765,7 +1765,7 @@ __STATIC_FORCEINLINE uint32_t __UQASX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uqasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uqasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1773,7 +1773,7 @@ __STATIC_FORCEINLINE uint32_t __UHASX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uhasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uhasx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1781,7 +1781,7 @@ __STATIC_FORCEINLINE uint32_t __SSAX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("ssax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("ssax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1789,7 +1789,7 @@ __STATIC_FORCEINLINE uint32_t __QSAX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("qsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1797,7 +1797,7 @@ __STATIC_FORCEINLINE uint32_t __SHSAX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("shsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("shsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1805,7 +1805,7 @@ __STATIC_FORCEINLINE uint32_t __USAX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("usax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("usax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1813,7 +1813,7 @@ __STATIC_FORCEINLINE uint32_t __UQSAX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uqsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uqsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1821,7 +1821,7 @@ __STATIC_FORCEINLINE uint32_t __UHSAX(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uhsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uhsax %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1829,7 +1829,7 @@ __STATIC_FORCEINLINE uint32_t __USAD8(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("usad8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("usad8 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1837,21 +1837,21 @@ __STATIC_FORCEINLINE uint32_t __USADA8(uint32_t op1, uint32_t op2, uint32_t op3)
 {
   uint32_t result;
 
-  __ASM volatile ("usada8 %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
+// __ASM volatile ("usada8 %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
   return(result);
 }
 
 #define __SSAT16(ARG1,ARG2) \
 ({                          \
   int32_t __RES, __ARG1 = (ARG1); \
-  __ASM ("ssat16 %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
+// __ASM ("ssat16 %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
   __RES; \
  })
 
 #define __USAT16(ARG1,ARG2) \
 ({                          \
   uint32_t __RES, __ARG1 = (ARG1); \
-  __ASM ("usat16 %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
+// __ASM ("usat16 %0, %1, %2" : "=r" (__RES) :  "I" (ARG2), "r" (__ARG1) ); \
   __RES; \
  })
 
@@ -1859,7 +1859,7 @@ __STATIC_FORCEINLINE uint32_t __UXTB16(uint32_t op1)
 {
   uint32_t result;
 
-  __ASM volatile ("uxtb16 %0, %1" : "=r" (result) : "r" (op1));
+// __ASM volatile ("uxtb16 %0, %1" : "=r" (result) : "r" (op1));
   return(result);
 }
 
@@ -1867,7 +1867,7 @@ __STATIC_FORCEINLINE uint32_t __UXTAB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("uxtab16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("uxtab16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1875,7 +1875,7 @@ __STATIC_FORCEINLINE uint32_t __SXTB16(uint32_t op1)
 {
   uint32_t result;
 
-  __ASM volatile ("sxtb16 %0, %1" : "=r" (result) : "r" (op1));
+// __ASM volatile ("sxtb16 %0, %1" : "=r" (result) : "r" (op1));
   return(result);
 }
 
@@ -1883,7 +1883,7 @@ __STATIC_FORCEINLINE uint32_t __SXTAB16(uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("sxtab16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("sxtab16 %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1891,7 +1891,7 @@ __STATIC_FORCEINLINE uint32_t __SMUAD  (uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("smuad %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("smuad %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1899,7 +1899,7 @@ __STATIC_FORCEINLINE uint32_t __SMUADX (uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("smuadx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("smuadx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1907,7 +1907,7 @@ __STATIC_FORCEINLINE uint32_t __SMLAD (uint32_t op1, uint32_t op2, uint32_t op3)
 {
   uint32_t result;
 
-  __ASM volatile ("smlad %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
+// __ASM volatile ("smlad %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
   return(result);
 }
 
@@ -1915,7 +1915,7 @@ __STATIC_FORCEINLINE uint32_t __SMLADX (uint32_t op1, uint32_t op2, uint32_t op3
 {
   uint32_t result;
 
-  __ASM volatile ("smladx %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
+// __ASM volatile ("smladx %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
   return(result);
 }
 
@@ -1928,9 +1928,9 @@ __STATIC_FORCEINLINE uint64_t __SMLALD (uint32_t op1, uint32_t op2, uint64_t acc
   llr.w64 = acc;
 
 #ifndef __ARMEB__   /* Little endian */
-  __ASM volatile ("smlald %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
+// __ASM volatile ("smlald %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
 #else               /* Big endian */
-  __ASM volatile ("smlald %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
+// __ASM volatile ("smlald %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
 #endif
 
   return(llr.w64);
@@ -1945,9 +1945,9 @@ __STATIC_FORCEINLINE uint64_t __SMLALDX (uint32_t op1, uint32_t op2, uint64_t ac
   llr.w64 = acc;
 
 #ifndef __ARMEB__   /* Little endian */
-  __ASM volatile ("smlaldx %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
+// __ASM volatile ("smlaldx %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
 #else               /* Big endian */
-  __ASM volatile ("smlaldx %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
+// __ASM volatile ("smlaldx %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
 #endif
 
   return(llr.w64);
@@ -1957,7 +1957,7 @@ __STATIC_FORCEINLINE uint32_t __SMUSD  (uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("smusd %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("smusd %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1965,7 +1965,7 @@ __STATIC_FORCEINLINE uint32_t __SMUSDX (uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("smusdx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("smusdx %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -1973,7 +1973,7 @@ __STATIC_FORCEINLINE uint32_t __SMLSD (uint32_t op1, uint32_t op2, uint32_t op3)
 {
   uint32_t result;
 
-  __ASM volatile ("smlsd %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
+// __ASM volatile ("smlsd %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
   return(result);
 }
 
@@ -1981,7 +1981,7 @@ __STATIC_FORCEINLINE uint32_t __SMLSDX (uint32_t op1, uint32_t op2, uint32_t op3
 {
   uint32_t result;
 
-  __ASM volatile ("smlsdx %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
+// __ASM volatile ("smlsdx %0, %1, %2, %3" : "=r" (result) : "r" (op1), "r" (op2), "r" (op3) );
   return(result);
 }
 
@@ -1994,9 +1994,9 @@ __STATIC_FORCEINLINE uint64_t __SMLSLD (uint32_t op1, uint32_t op2, uint64_t acc
   llr.w64 = acc;
 
 #ifndef __ARMEB__   /* Little endian */
-  __ASM volatile ("smlsld %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
+// __ASM volatile ("smlsld %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
 #else               /* Big endian */
-  __ASM volatile ("smlsld %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
+// __ASM volatile ("smlsld %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
 #endif
 
   return(llr.w64);
@@ -2011,9 +2011,9 @@ __STATIC_FORCEINLINE uint64_t __SMLSLDX (uint32_t op1, uint32_t op2, uint64_t ac
   llr.w64 = acc;
 
 #ifndef __ARMEB__   /* Little endian */
-  __ASM volatile ("smlsldx %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
+// __ASM volatile ("smlsldx %0, %1, %2, %3" : "=r" (llr.w32[0]), "=r" (llr.w32[1]): "r" (op1), "r" (op2) , "0" (llr.w32[0]), "1" (llr.w32[1]) );
 #else               /* Big endian */
-  __ASM volatile ("smlsldx %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
+// __ASM volatile ("smlsldx %0, %1, %2, %3" : "=r" (llr.w32[1]), "=r" (llr.w32[0]): "r" (op1), "r" (op2) , "0" (llr.w32[1]), "1" (llr.w32[0]) );
 #endif
 
   return(llr.w64);
@@ -2023,7 +2023,7 @@ __STATIC_FORCEINLINE uint32_t __SEL  (uint32_t op1, uint32_t op2)
 {
   uint32_t result;
 
-  __ASM volatile ("sel %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("sel %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -2031,7 +2031,7 @@ __STATIC_FORCEINLINE  int32_t __QADD( int32_t op1,  int32_t op2)
 {
   int32_t result;
 
-  __ASM volatile ("qadd %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qadd %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -2039,7 +2039,7 @@ __STATIC_FORCEINLINE  int32_t __QSUB( int32_t op1,  int32_t op2)
 {
   int32_t result;
 
-  __ASM volatile ("qsub %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
+// __ASM volatile ("qsub %0, %1, %2" : "=r" (result) : "r" (op1), "r" (op2) );
   return(result);
 }
 
@@ -2047,7 +2047,7 @@ __STATIC_FORCEINLINE  int32_t __QSUB( int32_t op1,  int32_t op2)
 #define __PKHBT(ARG1,ARG2,ARG3) \
 ({                          \
   uint32_t __RES, __ARG1 = (ARG1), __ARG2 = (ARG2); \
-  __ASM ("pkhbt %0, %1, %2, lsl %3" : "=r" (__RES) :  "r" (__ARG1), "r" (__ARG2), "I" (ARG3)  ); \
+// __ASM ("pkhbt %0, %1, %2, lsl %3" : "=r" (__RES) :  "r" (__ARG1), "r" (__ARG2), "I" (ARG3)  ); \
   __RES; \
  })
 
@@ -2055,9 +2055,9 @@ __STATIC_FORCEINLINE  int32_t __QSUB( int32_t op1,  int32_t op2)
 ({                          \
   uint32_t __RES, __ARG1 = (ARG1), __ARG2 = (ARG2); \
   if (ARG3 == 0) \
-    __ASM ("pkhtb %0, %1, %2" : "=r" (__RES) :  "r" (__ARG1), "r" (__ARG2)  ); \
+  // __ASM ("pkhtb %0, %1, %2" : "=r" (__RES) :  "r" (__ARG1), "r" (__ARG2)  ); \
   else \
-    __ASM ("pkhtb %0, %1, %2, asr %3" : "=r" (__RES) :  "r" (__ARG1), "r" (__ARG2), "I" (ARG3)  ); \
+  // __ASM ("pkhtb %0, %1, %2, asr %3" : "=r" (__RES) :  "r" (__ARG1), "r" (__ARG2), "I" (ARG3)  ); \
   __RES; \
  })
 #endif
diff --git a/components/net/at_device/emtc_bg36/bg36.h b/components/net/at_device/emtc_bg36/bg36.h
index e06d271..079ae8d 100644
--- a/components/net/at_device/emtc_bg36/bg36.h
+++ b/components/net/at_device/emtc_bg36/bg36.h
@@ -52,7 +52,7 @@ typedef struct emtc_socket_info_t
     int offset;
     char *buf;
     bool used_flag;
-}emtc_socket_info;
+}emtc_socket_info_;


 #define ATI "ATI\r"
